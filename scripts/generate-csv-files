#!/usr/bin/env python3

import argparse
import csv
import datetime
import glob
import json
import logging
import os
import re
import sys

# Use local libraries first
libdir = os.path.join(os.path.dirname(os.path.abspath(__file__)),'lib','python')
if not os.path.isdir(libdir):
	print(f"WARNING: '{libdir}' is not a directory. You may have problems.")
else:
	sys.path.insert(0, libdir)
	pass

from misc.ProcInfo import ProcInfo,me

class PSObject(ProcInfo):
	_num = 0
	_clean_space = re.compile('\s+|\s+[^a-zA-Z0-9]+\s+')
	SEARCHES = {
		"abortion": re.compile('\Wabortion'),
		"atheist": re.compile('\Watheis'),
		"barak": re.compile('\Wbarak\W'),
		"bill": re.compile('\Wbill\W'),
		"christian": re.compile('\W(christian|catholic|protistant|babtist|evangelical)'),
		"climate": re.compile('\Wclimate\W'),
		"clinton": re.compile('\Wclinton\W'),
		"consipracy": re.compile('\Wconspir'),
		"democrat": re.compile('\W(democrats?|dems)\W'),
		"donald": re.compile('\W(donald|don)\W'),
		"fake_news": re.compile('\Wfake.?news'),
		"fascism": re.compile('\W(fascism|faschists?)\W'),
		"foreigner": re.compile('\W(foreigners?|foriegners?)\W'),
		"fox_news": re.compile('\Wfox.?new'),
		"godwin": re.compile('\Wgodwin\W'),
		"hideki": re.compile('\Whideki\W'),
		"hillary": re.compile('\Whillary\W'),
		"hitler": re.compile('\Whitler\W'),
		"immigrants": re.compile('\W(immigrants?)\W'),
		"invasion": re.compile('\W(invasions?|invader?s?|invador?s?|invading)\W'),
		"jewish": re.compile('\W(jews?|jewish)'),
		"liberals": re.compile('\W(liberals?|libs?)\W'),
		"libtard": re.compile('\Wlibtards?\W'),
		"maga": re.compile('\W(make.?america.?great.?again|maga?)\W'),
		"mexican": re.compile('\Wmexican'),
		"muller": re.compile('\Wmuller'),
		"muslim": re.compile('\Wmuslim'),
		"mussolini": re.compile('\Wmussolini\W'),
		"nazi": re.compile('\Wnazis?m?\W'),
		"obama": re.compile('\Wobama\W'),
		"prochoice": re.compile('\Wpro.?choice'),
		"prolife": re.compile('\Wflat.?earth'),
		"prolife": re.compile('\Wpro.?lifer?'),
		"republican": re.compile('\Wrepublicans?\W'),
		"sarcasm": re.compile('(\/sa?r?|\\sa?r?)\W'),
		"socialis": re.compile('\Wsocialist?m?\W'),
		"south_america": re.compile('\Wsouth.?america'),
		"trump": re.compile('\Wtrump\W'),
		"undocumented": re.compile('\Wundocumented\W'),
		"vaccines": re.compile('\W(vax|vaccines?)\W'),
		"warming": re.compile('\Wglobal.?warm'),
	}
	SEARCHES_COLS = sorted(SEARCHES.keys())
	def __init__(self,json_str):
		self.num = PSObject._num
		PSObject._num += 1 # Not thread safe, FYI
		self.get_utc = self.get_int # for now
		self.json_str = json_str
		self.json = json.loads(json_str)
		self.had_flair = 0
		if self.get("author_flair_text",False):
			self.had_flair = 1
			pass
		self.body = self.get_str("body")
		if self.body is None:
			print(json_str)
			raise ValueError("No Body!")
		self.body = self.body.encode('ascii',errors='ignore').decode().lower()
		self.body = PSObject._clean_space.sub(' ',self.body)
		self.edited = self.get("edited")
		if self.edited is True:
			self.key_warn("edited","Expecting an int or \"False\", got \"True\", treating as \"False\".")
			self.edited = False
			pass
		if not self.edited:
			self.edited = 0
			pass
		self.data = {
			"id": self.get_str("id"),
			"top_id": None,
			"num_children":None,
			"distance_to_top":None,
			"parent_id": self.get_str("parent_id"),
			"subreddit": self.get_str("subreddit"),
			"subreddit_type": self.get_str("subreddit_type"),
			"all_awardings": self.list_size("all_awardings"),
			"archived": self.get_bool("archived"),
			"author": self.get_str("author"),
			"author_cakeday": self.get_bool("author_cakeday"),
			"had_flair": self.had_flair,
			"can_gild": self.get_bool("can_gild"),
			"collapsed": self.get_bool("collapsed"),
			"controversiality": self.get_int("controversiality"),
			"created_utc": self.get_utc("created_utc"),
			"distinguished": self.get_str("distinguished"),
			"edited": self.get_utc("edited"),
			"gilded": self.get_int("gilded"),
			"is_submitter": self.get_bool("is_submitter"),
			"link_id": self.get_str("link_id"),
			"locked": self.get_bool("locked"),
			"no_follow": self.get_bool("no_follow"),
			"quarantined": self.get_bool("quarantined"),
			"removal_reason": self.get_str("removal_reason"),
			"retrieved_on": self.get_utc("retrieved_on"),
			"score": self.get_int("score"),
			"stickied": self.get_bool("stickied"),
			"total_awards_received": self.get_int("total_awards_received"),
			"body": self.body,
			}

		for label,regex in PSObject.SEARCHES.items():
			self.data[label] = self.count_matches(regex)
			pass
		pass

	def count_matches(self,regex):
		return len(regex.findall(self.body))

	def key_warn(self,key,msg):
		if key in self.json:
			logging.warning(f"WARNING: Record #{self.num}; \"{key}\":{self.json[key]}: {msg}")
			return self

	def get(self,key,default=None):
		if key in self.json:
			return self.json[key]
		return default

	def get_type(self,key,vtype,default,none_ok=True):
		if key in self.json:
			val = self.json[key]
			if none_ok and val is None:
				return default
			if isinstance(val,vtype):
				return val
			else:
				if "int" == vtype.__name__:
					try:
						intval = int(val)
						return intval
					except ValueError as e:
						pass
					pass
				self.key_warn(key,f"Expecting a {vtype.__name__}. Returning \"{default}\".")
				pass
			pass
		return default

	def get_bool(self,key,default=None,none_ok=True):
		#return self.get_type(key,bool,default,none_ok)
		if key in self.json and self.json[key]:
			return 1
		return None

	def get_str(self,key,default=None,none_ok=True):
		return self.get_type(key,str,default,none_ok)

	def get_int(self,key,default=None,none_ok=True):
		return self.get_type(key,int,default,none_ok)

	def get_utc(self,key,default=None,none_ok=True):
		try:
			intval = int(self.get_utc())
			dt = datetime.datetime.utcfromtimestamp(inval)
			utc = dt.trftime("%Y-%m-%d-%H:%M:%S")
		except ValueError as e:
			if key not in self.json:
				return self.json[key]
			return default
			pass
		return utc

	def list_size(self,key):
		if key in self.json:
			if isinstance(self.json[key],list):
				return len(self.json[key])
			else:
				self.key_warn(key,"Expecting a list. Will assume enmpty list.")
				pass
			pass
		return 0
class PSFile(ProcInfo):
	def __init__(self,parent,filename):
		self.parent = parent
		self.args = self.parent.args
		if not os.path.exists(filename):
			raise ValueError(f"No such file or directory: '{filename}'")
		if not os.path.isfile(filename):
			raise ValueError(f"Not a regular file: '{filename}'")
		self.filename = filename
		self.size = os.path.getsize(filename)
		self.bytes_read = 0
		self.t0 = self.now()
		self.tn = self.now()
		self.cols = self.parent.cols
		pass

	def process(self):
		self.ids = {}
		if self.args.verbosity > 0:
			logging.info(f"Loading '{self.filename}'")
			pass
		if self.args.output_dir is not None:
			output_base = os.path.join(self.args.output_dir,os.path.basename(self.filename))
			pass
		else:
			output_base = self.filename
			pass
		self.full_path = os.path.realpath(self.filename)
		self.basename, self.extension = os.path.splitext(output_base)
		self.meta_filename = f"{self.basename}-meta.csv"
		self.body_filename = f"{self.basename}-bodies.csv"
		logging.info(f"Saving Meta Information to .. '{self.meta_filename}'")
		logging.info(f"Saving Bodies to ............ '{self.body_filename}'")
		self.meta_fh = open(self.meta_filename, 'w')
		self.meta_writer = csv.writer(self.meta_fh)
		self.meta_writer.writerow(self.cols)
		self.body_fh = open(self.body_filename, 'w')
		self.body_writer = csv.writer(self.body_fh)
		self.body_writer.writerow(["comment_id","body"])
		self.record_count = 0
		self.dupes = 0
		self.t0 = self.now()
		self.bytes_read = 0
		with open(self.filename, 'r') as fh:
			for line in fh:
				bytes_read = len(line)
				self.record_count += 1
				self.parent.record_count += 1
				self.bytes_read += bytes_read
				ps_obj = PSObject(line)
				row = []
				ps_obj_id = ps_obj.data["id"]
				self.body_writer.writerow([ps_obj.data["id"],ps_obj.data["body"]])
				if 0 == self.parent.record_count % self.args.record_tick_check:
					lapsed = self.elapsed(t0=self.tn)
					if lapsed >= self.args.progress_secs:
						self.tn = self.now()
						self.show_progress()
					else:
						if self.args.debug:
							logging.debug(f"Nope: {self.pnum(self.record_count)} recs, {self.psecs(lapsed)} lapsed, min={self.psecs(self.args.progress_secs)}")
							pass
						pass
					pass
				if self.args.dedupe:
					if ps_obj_id in self.ids:
						self.ids[ps_obj_id] += 1
						self.dupes += 1
						self.current_dupes += 1
					else:
						self.ids[ps_obj_id] = 0
						pass
				for key in self.cols:
					row.append(ps_obj.data[key])
					pass
				try:
					self.meta_writer.writerow(row)
				except:
					logging.error(f"ERROR writing row {row}")
					raise
				pass
			pass
		self.body_fh.close()
		self.meta_fh.close()
		self.show_progress(last=True)
		pass

	def show_progress(self, last=False):
		num = self.pnum(self.record_count)
		rate = self.rate(self.bytes_read,total=self.size)
		record_rate = self.rate(self.record_count)
		read = self.pbytes(self.bytes_read)
		togo = self.pbytes(self.size)
		time = self.psecs()
		meta_size = os.path.getsize(self.meta_filename)
		body_size = os.path.getsize(self.body_filename)
		percent = "%0.1f%%" %(self.bytes_read * 100.0/self.size)
		logging.info(f"{self.filename}: {time} for {num} comments at {record_rate}. Completion: {percent}[{read}/{togo}] {rate}")
		self.parent.show_progress()
		return self
	pass

class PSProcessor(ProcInfo):
	def __init__(self,args):
		super().__init__()
		self.args = args
		self.t0 = self.now()
		self.tn = self.now()
		self.size = 0
		self.record_count = 0
		self.errs = []
		filenames = []
		glob_files = []
		for filename in self.args.files:
			if "*" in filename or "?" in filename or "[" in filename:
				globs = 0
				for fname in glob.glob(filename):
					globs += 1
					filenames.append(fname)
					pass
				if globs < 1:
					self.errs.append(f"Found no files for '{filename}'")
					pass
				pass
			else:
				filenames.append(filename)
				pass
			pass
		for fname in filenames:
			if not os.path.exists(fname):
				self.errs.append(f"'{fname}' does not exist.")
			elif not os.path.isfile(fname):
				self.errs.append(f"'{fname}' is not a regular file.")
			else:
				self.size += os.path.getsize(fname)
				pass
			pass
		if(len(self.errs)):
			for err in self.errs:
				print(f"ERROR: {err}")
				pass
			print("ABORTING")
			sys.exit(1)
			pass
		self.args.files = filenames
		self.psfiles = []
		self.cols = [
			"id",
			"top_id",
			"parent_id",
			"subreddit",
			"subreddit_type",
			"created_utc",
			"score",
			"stickied",
			"retrieved_on",
			"author",
			"all_awardings",
			"archived",
			"author_cakeday",
			"had_flair",
			"can_gild",
			"collapsed",
			"controversiality",
			"distinguished",
			"edited",
			"gilded",
			"is_submitter",
			"link_id",
			"locked",
			"no_follow",
			"quarantined",
			"removal_reason",
			"total_awards_received",
			]
		self.cols += PSObject.SEARCHES_COLS
		#self.cols += ["body"]
		self.log_filename = None
		if self.args.output_dir is not None:
			self.log_filename = os.path.join(self.args.output_dir,self.unique_log_filename)
		else:
			self.log_filename = self.unique_log_filename
			pass
		loglevel = logging.INFO
		if self.args.debug:
			loglevel = logging.DEBUG
			pass
		logging.basicConfig(
			level=loglevel,
			format='%(asctime)s %(levelname)-8s %(message)s',
			datefmt='%Y-%m-%d %H:%M:%S',
			filename=self.log_filename,
			filemode='w'
			)
		# define a Handler which writes INFO messages or higher to the sys.stderr
		console = logging.StreamHandler()
		console.setLevel(logging.INFO)
		# set a format which is simpler for console use
		#formatter = logging.Formatter('%(asctime)-12s: %(levelname)-8s %(message)s')
		# tell the handler to use this format
		#console.setFormatter(formatter)
		# add the handler to the root logger
		logging.getLogger('').addHandler(console)

		logging.info(f"Writing to log file '{self.log_filename}'")
		for filename in filenames:
			self.psfiles.append(PSFile(self,filename))
		pass

	def process(self):
		logging.info(f"Working on {len(self.psfiles)} files.")
		for psfile in self.psfiles:
			logging.info(f" - {self.pbytes(psfile.size)} '{psfile.filename}'")
			pass
		logging.info(f" - {self.pbytes(self.size)} TOTAL")
		logging.info("SETTING:")
		logging.info(f" - Will check if at least {self.psecs(self.args.progress_secs)} have passed every {self.pnum(self.args.record_tick_check)} records.")
		if self.args.output_file is not None:
			if self.args.output_dir is not None:
				logging.error("ERROR: if --output-file is present, --output-dir cannot be.")
				sys.exit(1)
				return self
			pass
		for psfile in self.psfiles:
			psfile.process()
			pass
		return self

	def show_progress(self):
		num = self.pnum(self.record_count)
		self.bytes_read = 0
		for psfile in self.psfiles:
			self.bytes_read += psfile.bytes_read
			pass
		rate = self.rate(self.bytes_read,total=self.size)
		record_rate = self.rate(self.record_count)
		read = self.pbytes(self.bytes_read)
		togo = self.pbytes(self.size)
		time = self.psecs()
		percent = "%0.1f%%" %(self.bytes_read * 100.0/self.size)
		logging.info(f"TOTAL:   {time} for {num} comments at {record_rate}. Completion: {percent}[{read}/{togo}] {rate}")
		return self
	pass

def main():
	parser = argparse.ArgumentParser(description='Build info about data from PushShift.io files.')
	parser.add_argument('files', nargs='+', help="Files to process.")
	parser.add_argument('--debug', '-d', default=False, action='store_true', dest='debug', help="Turn debugging on.")
	parser.add_argument('--de-dupe', '-dd', default=False, action='store_true', dest='dedupe', help="Remove duplicates.")
	parser.add_argument('--output-file', '-o', default=None, dest='output_file', help="Basename of the output CSV.")
	parser.add_argument('--output-dir', '-D', default=None, dest='output_dir', help="Write multiple output files to this directory.")
	parser.add_argument('--record-tick-check', '-rt', default=10, dest='record_tick_check', type=int,help="Number of THOUSAND records before checking if --progress-secs has passed.")
	parser.add_argument('--progress-secs', '-ps', default=30, dest='progress_secs', type=int,help="Seconds before showing progress.")
	parser.add_argument('--quiet', '-q', default=0, dest='quiet', action='count',help="Decrease verbosity. Can have multiple.")
	parser.add_argument('--verbose', '-v', default=0, dest='verbosity', action='count',help="Increase verbosity. Can have multiple.")

	args = parser.parse_args()
	args.verbosity = 1 + args.verbosity - args.quiet
	args.record_tick_check = args.record_tick_check * 1000
	psfile = PSProcessor(args)
	psfile.process()
	pass

if __name__ == "__main__":
	main()
	sys.exit(0)
	pass
