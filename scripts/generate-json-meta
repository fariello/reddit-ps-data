#!/usr/bin/env python3

import argparse
import json
import os
import sys
import time
import datetime
import subprocess
import pprint
import glob

# I just like having a class with "my" info
class MyInfo:
	# Not super precise, but goot enough for government work.
	_t0 = datetime.datetime.now()
	_cwd = os.getcwd()

	def init_class(self):
		pre = "pico nano micro milli n kilo mega giga tera peta exa zetta yotta".split()
		MyInfo.prefixes = pre
		start = -12
		MyInfo.pre_min = 1.0 * 10**start
		MyInfo.pre_max = MyInfo.pre_min
		MyInfo.prefix2val = {}
		MyInfo.val2long = {}
		MyInfo.val2short = {}
		MyInfo.magnitutes = []
		for prefix in pre:
			short = prefix[0].upper()
			current = 1.0 * 10**start
			MyInfo.pre_max = current
			MyInfo.magnitutes.append(current)
			MyInfo.prefix2val[prefix] = current
			MyInfo.prefix2val[short] = current
			MyInfo.val2long[current] = prefix
			MyInfo.val2short[current] = short
			start += 3
			pass
		pass

	def __init__(self):
		self.t0 = MyInfo._t0
		self.initial_dir = MyInfo._cwd
		# This is the "real" filename. If it was called with a symlink, this
		# will point to the target file not the symlink
		self.real_filename = os.path.realpath(__file__)
		# This is the filename of the called script which may not be the same
		# as the "real" filename
		self.filename = os.path.abspath(__file__)
		self.basename = os.path.basename(__file__)
		self.dir = os.path.dirname(os.path.abspath(__file__))
		self.simestamp = time.strftime("%Y%m%d-%H%M%S")
		self.secs_per_minute = 60
		self.secs_per_hour = 3600
		self.secs_per_day = self.secs_per_hour * 24
		self.secs_per_week = self.secs_per_day * 7
		self.secs_per_year = self.secs_per_day * 365.25
		self.secs_per_month = self.secs_per_year / 12
		self.init_class()
		pass

	def now(self):
		return datetime.datetime.now()

	def elapsed(self,tn=None,t0=None):
		if tn is None:
			tn = self.now()
			pass
		if t0 is None:
			t0 = self.t0
			pass
		return (tn - t0).total_seconds()

	def pbytes(self,num):
		if num >= 1000000000000000:
			return "%0.2fPB" %(num / 1000000000000000)
		if num >= 1000000000000:
			return "%0.2fTB" %(num / 1000000000000)
		if num >= 1000000000:
			return "%0.2fGB" %(num / 1000000000)
		if num >= 1000000:
			return "%0.2fMB" %(num / 1000000)
		if  num >= 1000:
			return "%0.2fKB" %(num / 1000)
		return "%3s Bytes" %(num)

	def pnum(self,num):
		if num >= 1000000000000000:
			return "%0.2fP" %(num / 1000000000000000)
		if num >= 1000000000000:
			return "%0.2fT" %(num / 1000000000000)
		if num >= 1000000000:
			return "%0.2fG" %(num / 1000000000)
		if num >= 1000000:
			return "%0.2fM" %(num / 1000000)
		if  num >= 1000:
			return "%0.2fK" %(num / 1000)
		return "%3s " %(num)

	def psecs(self, secs=None):
		if secs is None:
			secs = self.elapsed()
			pass
		if secs >= self.secs_per_year:
			secs = secs / self.secs_per_year
			return f"{secs:0.2f} Years"
		if secs >= self.secs_per_month:
			secs = secs / self.secs_per_month
			return f"{secs:0.2f} Months"
		if secs >= self.secs_per_week:
			secs = secs / self.secs_per_week
			return f"{secs:0.2f} Weeks"
		if secs >= self.secs_per_day:
			secs = secs / self.secs_per_day
			return f"{secs:0.2f} Days"
		if secs >= self.secs_per_hour:
			secs = secs / self.secs_per_hour
			return f"{secs:0.2f} Hours"
		if secs >= self.secs_per_minute:
			secs = secs / self.secs_per_minute
			return f"{secs:0.2f} Minutes"
		return f"{secs:0.2f} Secs"

	def rate(self,count,tn=None,t0=None,total=None,append=""):
		secs = self.elapsed(tn,t0)
		rate = count / secs
		if total is not None:
			secs_remain = (total - count) / rate
			percent = f"{count * 100 / total:0.2f}"
			estimate = self.psecs(secs_remain)
			estimate = f" ({self.pnum(count)} of {self.pnum(total)} {percent}% ~ {estimate} Remaining)"
		else:
			estimate = ""
			pass
		if rate <= 1.0 / self.secs_per_year:
			rate = rate * self.secs_per_year
			unit = "Year"
		elif rate <= 1.0 / self.secs_per_month:
			rate = rate * self.secs_per_month
			unit = "Month"
		elif rate <= 1.0 / self.secs_per_week:
			rate = rate * self.secs_per_week
			unit = "Week"
		elif rate <= 1.0 / self.secs_per_hour:
			rate = rate * self.secs_per_hour
			unit = "Hour"
		elif rate <= 1.0 / self.secs_per_minute:
			rate = rate * self.secs_per_minute
			unit = "Minute"
		elif rate >= 1000:
			rate = self.pnum(rate)
			return f"{rate}/Sec{estimate}"
		else:
			unit = "Second"
			pass
		return f"{rate:0.2f}/{unit}{estimate}"
	pass
me = MyInfo()


class ValueTypeTracker:
	def __init__(self,attribute_obj,type_str,name=None):
		self.attr = attribute_obj
		self.str = type_str
		if name is None:
			self.name = self.str
		else:
			self.name = name
			pass
		self.count = 0
		self.max_unique = 250
		self.too_many = False
		self.unique_value_count = 0
		self.unique_values = {}
		pass
	def set_too_many(self):
		self.too_many = True
		self.unique_values = "Too Many"
		self.unique_value_count = None
		return self
	def track_unique(self,value):
		if self.too_many:
			return self
		if value not in self.unique_values:
			if self.unique_value_count >= self.max_unique:
				return self.set_too_many()
			self.unique_values[value] = 0
			self.unique_value_count += 1
			pass
		self.unique_values[value] += 1
		return self
	def track(self,value):
		self.count += 1
		return self
	def merge(self,other):
		self.count += other.count
		return self
	def to_dict(self):
		return {"count":self.count }
	pass

class NoneValueTypeTracker(ValueTypeTracker):
	def __init__(self,attribute_obj,type_str):
		super().__init__(attribute_obj,type_str,"null")
		pass
	pass

class UniqueValueTypeTracker(ValueTypeTracker):
	def __init__(self,attribute_obj,type_str,name=None,max_unique=250):
		super().__init__(attribute_obj,type_str,name)
		self.max_unique = max_unique
		self.size = 0
		pass
	def track(self,value):
		self.count += 1
		return self.track_unique(value)
	def merge(self,other):
		super().merge(other)
		if other.too_many:
			return self.set_too_many()
		if self.too_many:
			return self
		for value,count in other.unique_values.items():
			if value not in self.unique_values:
				self.unique_values[value] = 0
				pass
			self.unique_values[value] += other.unique_values[value]
			pass
		if len(self.unique_values.keys()) > self.max_unique:
			self.set_too_many()
		return self
	def to_dict(self):
		return {"count":self.count,"unique_values":self.unique_values}
	pass
class BoolValueTypeTracker(UniqueValueTypeTracker):
	pass
class NumericValueTypeTracker(UniqueValueTypeTracker):
	def __init__(self,attribute_obj,type_str,name=None):
		super().__init__(attribute_obj,type_str,name)
		self.min = None
		self.max = None
		self.sum = 0
		pass
	def track(self,value,num=None):
		super().track(value)
		if num is None:
			num = value
			pass
		self.sum += num
		if self.max is None or num > self.max:
			self.max = num
			pass
		if self.min is None or num < self.min:
			self.min = num
			pass
		return self
	def merge(self,other):
		super().merge(other)
		self.sum += other.sum
		if self.min is None or other.min < self.min:
			self.min = other.min
			pass
		if self.max is None or other.max > self.max:
			self.max = other.max
			pass
		return self
	def to_dict(self):
		return {
			"count":self.count,
			"sum":self.sum,
			"min":self.min,
			"max":self.max,
			"mean": self.sum / self.count,
			"unique_values": self.unique_values,
			}
	pass
class IntValueTypeTracker(NumericValueTypeTracker):
	pass
class FloatValueTypeTracker(NumericValueTypeTracker):
	pass
class LenthyValueTypeTracker(NumericValueTypeTracker):
	def track(self,value):
		return super().track(len(value))
	def to_dict(self):
		return {
			"count":self.count,
			"min_length":self.min,
			"max_length":self.max,
			"mean_length": self.sum / self.count,
			"unique_lengths": self.unique_values,
			}
	pass
class StrValueTypeTracker(NumericValueTypeTracker):
	def track(self,value):
		return super().track(value,len(value))
	def to_dict(self):
		return {
			"count":self.count,
			"length_sum":self.sum,
			"min_length":self.min,
			"max_length":self.max,
			"mean_length": self.sum / self.count,
			"unique_values": self.unique_values,
			}
	pass
class ListValueTypeTracker(LenthyValueTypeTracker):
	_name = "array"
	pass
class DictValueTypeTracker(LenthyValueTypeTracker):
	def track(self,value):
		super().track(value.keys())
		pass
	pass

class AttributeTracker:
	_value_type_map = {
		"list": ListValueTypeTracker,
		"bool": BoolValueTypeTracker,
		"dict": DictValueTypeTracker,
		"float": FloatValueTypeTracker,
		"int": IntValueTypeTracker,
		"NoneType": NoneValueTypeTracker,
		"str": StrValueTypeTracker,
		}
	def __init__(self,parent,name):
		self.parent = parent
		self.name = name
		self.count = 0
		self.value_trackers = {}
		pass

	def reset(self):
		self.parent = None
		self.count = 0
		self.value_trackers = {}
		pass

	def get_type_tracker(self,type_str):
		if type_str not in AttributeTracker._value_type_map:
			raise ValueError(f"Unknown type='{type_str}' for value='{value}'")
		if type_str not in self.value_trackers:
			self.value_trackers[type_str] = AttributeTracker._value_type_map[type_str](self,type_str)
			pass
		return self.value_trackers[type_str]

	def track(self,value):
		self.count += 1
		self.get_type_tracker(type(value).__name__).track(value)
		return self

	def merge(self,other):
		self.count += other.count
		for type_str,type_tracker in other.value_trackers.items():
			my_tracker = self.get_type_tracker(type_str)
			my_tracker.merge(type_tracker)
			pass
		pass

	def to_dict(self):
		value_trackers = {}
		for type_str,type_tracker in self.value_trackers.items():
			value_trackers[type_tracker.name] = type_tracker.to_dict()
			pass
		return {
			"count":self.count,
			"value_types":value_trackers,
			"prevalence": self.count / self.parent.object_count,
			}

class RedditBase(MyInfo):
	def __init__(self):
		super().__init__()
		self.object_count = 0
		self.files = []
		self.attribute_trackers = {}
		self.bytes_total = 0
		self.bytes_read = 0
		self.t0 = datetime.datetime.now()
		pass
	def get_attribute_tracker(self,attribute_name):
		if attribute_name not in self.attribute_trackers:
			self.attribute_trackers[attribute_name] = AttributeTracker(self,attribute_name)
			pass
		return self.attribute_trackers[attribute_name]

	def track_attrib(self,attribute_name,attribute_value):
		self.get_attribute_tracker(attribute_name).track(attribute_value)
		return self

	def to_dict(self):
		attrs = {}
		for attribute_name,attribute_tracker in self.attribute_trackers.items():
			attrs[attribute_name] = attribute_tracker.to_dict()
			pass
		my_dict = {
			"attributes":attrs,
			"object_count": self.object_count,
			"input_files": self.files,
			"seconds": ( datetime.datetime.now() - self.t0 ).total_seconds(),
			"bytes_total": self.bytes_total,
			"bytes_read": self.bytes_read,
			"t0": self.t0.strftime("%y-%m-%d %H:%M:%S"),
			}
		return my_dict

	def save_json_meta(self,last_update=False):
		print(f"Saving info to '{self.json_file}'")
		with open(self.json_file,'w') as fh:
			obj = self.to_dict()
			if last_update:
				obj["finished"] = True
				pass
			try:
				json.dump(obj,fh,indent=2,sort_keys=True)
			except:
				print(obj)
				raise
			pass
		return self

	pass

class RedditFile(RedditBase):
	"""Class to process one file in the format found on pushshift.io.

	It will create a JSON-based meta file which contains information on
	all of the types of attributes found for the particular type of file
	(tested on RC (Reddit Comments) and RS (Reddit Submission) files.

	When finised, it will pass the information to the parent so that an
	aggregate meta file for all files can be created.
	"""
	_ext2tool = {
		".bz2": ["bunzip2","-q","-c"],
		".zst": ["unzstd","-q","-c"],
		".gz": ["gunzip","-q","-c"],
		".xz": ["gunxz","-q","-c"],
		}
	def __init__(self,parent,filename):
		super().__init__()
		self.parent = parent
		self.args = parent.args
		self.filename = filename
		self.short_filename = os.path.basename(filename)
		self.files = [self.short_filename]
		self.full_filename = os.path.realpath(os.path.join(filename))
		self.base, self.extension = os.path.splitext(self.full_filename)
		print(f" - Will process '{self.full_filename}'.")
		self.file_size = os.path.getsize(self.full_filename)
		if self.extension in RedditFile._ext2tool:
			self.cmd = RedditFile._ext2tool[self.extension]
		else:
			self.cmd = None
			self.bytes_total = self.file_size
			pass
		self.ext = self.extension
		self.json_file = f"{self.base}-meta2.json"
		self.processed = False
		self.last_save_count = 0
		pass

	def process_line(self,line):
		"""Process one input line. The pushshift.io files all seem to

		have a psuedo-JSON format where each line is one object.

		This function processes one such input and adds its info to the
		meta object.

		"""
		self.bytes_read += len(line)
		self.object_count += 1
		self.parent.record_count += 1
		if 0 == self.object_count % self.args.min_record_check:
			self.update_progress()
			pass
		try:
			self.count_attribs(json.loads(line))
		except json.decoder.JSONDecodeError as e:
			print("ERROR on line %s" %(self.object_count))
			print("Line:\n%s" %(line))
			self.update_progress()
			raise e
		return self

	def process(self):
		""" Process the whole file, one line at a time."""
		self.t0 = datetime.datetime.now()
		# If we need to un-compress the file, do so using the appropriate
		# command and read in from STDIN.
		if self.cmd:
			return self.proc_from_cmd()
		# Otherwise, just read the file directly and process.
		with open(self.full_filename,'r') as fh:
			return self._proc_fh(fh)
		return self
	pass

	def proc_from_cmd(self):
		""" Process STDOUT from a command (e.g., uncompress) """
		# Don't process a file twice. Not sure why I thought I needed to check,
		# but much of this code is cut-and-paste from previous code used in
		# other research.
		if self.processed:
			return self
		# Prepare the command (append the file name to process
		self.cmd = self.data["subprocess"]
		self.cmd.append(self.data["file_name"])
		if args.verbosity > 0:
			print(f"Running: {self.cmd}")
			pass
		proc = subprocess.Popen(
			self.cmd,
			stdout=subprocess.PIPE,
			universal_newlines=True
			)
		# Now process like any other file handle.
		return self._proc_fh(proc.stdout)

	def _proc_fh(self,fh):
		""" Process the file once we have the file handle from which to read."""
		for line in fh:
			try:
				self.process_line(line)
			except json.decoder.JSONDecodeError as e:
				err = f"ERROR in JSON format, aborting load for '{self.filename}'."
				print(err)
				self.parent.errors(err)
				return self.save_progress(last_update=True)
			pass
		return self.save_progress(last_update=True)

	def count_attribs(self,obj):
		for key,value in obj.items():
			#size = len(str(value))
			self.track_attrib(key,value)
			#self.track_attrib(key,value,size)
			pass
		pass

	def update_progress(self):
		count = self.object_count
		args = self.args
		tn = datetime.datetime.now()
		secs = (tn - self.parent.tn).total_seconds()
		if secs < args.min_save_secs:
			return self
		if args.save_progress_each and count - self.last_save_count >= args.save_progress_each:
			self.last_save_count = count
			self.save_progress()
			pass
		self.parent.tn = tn
		num = self.pnum(count)
		if not self.cmd:
			rate = self.rate(self.bytes_read,total=self.file_size,append="bytes")
		else:
			rate = self.rate(count,total=args.num_records)
			pass
		print(f"{self.short_filename}: {self.psecs()} for {num} records. {rate}")
		self.parent.update_progress()
		return self


	def save_progress(self, last_update=False):
		if last_update:
			#self.parent.update_progress()
			self.save_json_meta(last_update=last_update)
			self.parent.merge_file_trackers(self)
		elif self.object_count - self.last_save_count >= self.args.save_progress_each:
			self.last_save_count = self.object_count
			pass
		return self


class RedditFileGroup(RedditBase):

	def __init__(self,args):
		super().__init__()
		self.args = args
		self.directory = args.directory
		self.json_file = args.outfile
		self.tn = datetime.datetime.now()
		self.file_count = 0
		self.total_size = 0
		self.record_count = 0
		self.file_objs = []
		self.errs = []
		self.jso_file = args.outfile
		for fname in self.args.files:
			if "*" in fname:
				for fname in glob.glob(fname):
					self.prep_file(fname)
					pass
				pass
			else:
				self.prep_file(fname)
				pass
			pass
		if self.directory is not None:
			self.prep_dir(self.directory)
			pass
		print(f"Will process a total of {self.file_count} files for {self.pbytes(self.total_size)}.")
		pass

	def errors(self,msg):
		self.errs.append(msg)
		return self

	def merge_file_trackers(self,psfmeta):
		self.files = []
		self.object_count = 0
		self.bytes_total = 0
		self.bytes_read = 0
		del(self.attribute_trackers)
		self.attribute_trackers = {}
		for file_tracker in self.file_objs:
			self.object_count += file_tracker.object_count
			self.bytes_total += file_tracker.bytes_total
			self.bytes_read += file_tracker.bytes_read
			self.files += file_tracker.files
			for attribute_name, tracker in file_tracker.attribute_trackers.items():
				self.get_attribute_tracker(attribute_name).merge(tracker)
				pass
			pass
		self.save_json_meta()
		return self

	def prep_file(self,filename):
		psfile = RedditFile(self,filename)
		self.file_objs.append(psfile)
		self.file_count += 1
		self.total_size += psfile.file_size
		pass

	def prep_dir(self,directory):
		if not os.path.exists(self.directory):
			print("ERROR: Could not find '%s'" %(self.directory))
			exit()
			pass
		print(f"Working on '{directory}' ...")
		for (dirpath,dirnames,filenames) in os.walk(directory):
			for filename in filenames:
				full_filename = os.path.realpath(os.path.join(dirpath,filename))
				self.prep_file(full_filename)
			pass
		pass

	def process(self):
		self.t0 = datetime.datetime.now()
		for ofile in self.file_objs:
			ofile.process()
			pass
		pass

	def update_progress(self):
		if len(self.file_objs) < 2:
			return self
		can_know_input_bytes = True
		self.bytes_read = 0
		self.object_count = 0
		for psfmeta in self.file_objs:
			self.bytes_read += psfmeta.bytes_read
			self.object_count += psfmeta.object_count
			if psfmeta.cmd:
				can_know_input_bytes = False
				pass
			pass
		args = self.args
		tn = datetime.datetime.now()
		secs = (tn - self.tn).total_seconds()
		num = self.pnum(self.object_count)
		if can_know_input_bytes:
			rate = self.rate(self.bytes_read,total=self.total_size,append="bytes")
		else:
			rate = self.rate(self.object_count,total=args.num_records)
			pass
		print(f"Overall: {self.psecs()} for {num} records. {rate}")
		return self


def main():
	parser = argparse.ArgumentParser(description='Build info about data from pushshift.io Reddit files.')
	parser.add_argument('files', nargs='+', help="Files to process.")
	parser.add_argument('--debug', '-d', default=False, action='store_true', dest='debug', help="Turn debugging on.")
	parser.add_argument('--dir', '-D', default=None, dest='directory', help="Process files in this directory.")
	parser.add_argument('--min-record-check', '-mr', default=100, dest='min_record_check', type=int,help="To speed things up, we only check if --min-save-secs after this many thousand records. Default: 5 (i.e., 5,000)")
	parser.add_argument('--min-save-secs', '-ms', default=60, dest='min_save_secs', type=int,help="The minimum number of secods which must pass before we save progress. Default: 10")
	parser.add_argument('--num-records', '-nr', default=None, dest='num_records', type=int,help="Number of records expected. Used to estimate time remaining.")
	parser.add_argument('--out', '-o', default=None, dest='outfile', help="Save aggregated JSON meta data to this file.")
	parser.add_argument('--quiet', '-q', default=0, dest='quiet', action='count',help="Decrease verbosity. Can have multiple.")
	parser.add_argument('--save-progress-each', '-sp', default=10, dest='save_progress_each', type=int,help="Number of --min-record-check for printing what we have so far with attribute information.")
	parser.add_argument('--verbose', '-v', default=0, dest='verbosity', action='count',help="Increase verbosity. Can have multiple.")

	args = parser.parse_args()
	args.verbosity = 1 + args.verbosity - args.quiet
	args.min_record_check = args.min_record_check * 1000
	if args.save_progress_each is not None:
		args.save_progress_each = args.save_progress_each * args.min_record_check
		pass
	if args.verbosity > 0:
		print(f"Settings:")
		print(f" - {me.pnum(args.min_record_check)} lines before checking for save or print.")
		print(f" - {me.psecs(args.min_save_secs)} minimum between progress output.")
		print(f" - Save/print after {me.pnum(args.save_progress_each)} records if {me.psecs(args.min_save_secs)} have passed.")
		pass
	args.timestr = time.strftime("%Y%m%d-%H%M%S")
	pinfo = RedditFileGroup(args)
	if len(pinfo.files) > 1:
		if args.outfile is None:
			print("ERROR: --out required if more than one file.")
			exit(1)
			pass
		pass
	pinfo.process()
	pass

if __name__ == "__main__":
	main()
	sys.exit(0)
	pass
