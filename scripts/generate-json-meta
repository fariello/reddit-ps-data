#!/usr/bin/env python3

import argparse
import json
import os
import sys
import time
import datetime
import subprocess
import pprint
import glob

# Use local libraries first
libdir = os.path.join(os.path.dirname(os.path.abspath(__file__)),'lib','python')
if not os.path.isdir(libdir):
	print(f"WARNING: '{libdir}' is not a directory. You may have problems.")
else:
	sys.path.insert(0, libdir)
	pass

from misc.MyInfo import MyInfo

class ValueTypeTracker:
	def __init__(self,attribute_obj,type_str,name=None):
		self.attr = attribute_obj
		self.str = type_str
		if name is None:
			self.name = self.str
		else:
			self.name = name
			pass
		self.count = 0
		self.max_unique = 250
		self.too_many = False
		self.unique_value_count = 0
		self.unique_values = {}
		pass
	def set_too_many(self):
		self.too_many = True
		self.unique_values = "Too Many"
		self.unique_value_count = None
		return self
	def track_unique(self,value):
		if self.too_many:
			return self
		if value not in self.unique_values:
			if self.unique_value_count >= self.max_unique:
				return self.set_too_many()
			self.unique_values[value] = 0
			self.unique_value_count += 1
			pass
		self.unique_values[value] += 1
		return self
	def track(self,value):
		self.count += 1
		return self
	def merge(self,other):
		self.count += other.count
		return self
	def to_dict(self):
		return {"count":self.count }
	pass

class NoneValueTypeTracker(ValueTypeTracker):
	def __init__(self,attribute_obj,type_str):
		super().__init__(attribute_obj,type_str,"null")
		pass
	pass

class UniqueValueTypeTracker(ValueTypeTracker):
	def __init__(self,attribute_obj,type_str,name=None,max_unique=250):
		super().__init__(attribute_obj,type_str,name)
		self.max_unique = max_unique
		self.size = 0
		pass
	def track(self,value):
		self.count += 1
		return self.track_unique(value)
	def merge(self,other):
		super().merge(other)
		if other.too_many:
			return self.set_too_many()
		if self.too_many:
			return self
		for value,count in other.unique_values.items():
			if value not in self.unique_values:
				self.unique_values[value] = 0
				pass
			self.unique_values[value] += other.unique_values[value]
			pass
		if len(self.unique_values.keys()) > self.max_unique:
			self.set_too_many()
		return self
	def to_dict(self):
		return {"count":self.count,"unique_values":self.unique_values}
	pass
class BoolValueTypeTracker(UniqueValueTypeTracker):
	pass
class NumericValueTypeTracker(UniqueValueTypeTracker):
	def __init__(self,attribute_obj,type_str,name=None):
		super().__init__(attribute_obj,type_str,name)
		self.min = None
		self.max = None
		self.sum = 0
		pass
	def track(self,value,num=None):
		super().track(value)
		if num is None:
			num = value
			pass
		self.sum += num
		if self.max is None or num > self.max:
			self.max = num
			pass
		if self.min is None or num < self.min:
			self.min = num
			pass
		return self
	def merge(self,other):
		super().merge(other)
		self.sum += other.sum
		if self.min is None or other.min < self.min:
			self.min = other.min
			pass
		if self.max is None or other.max > self.max:
			self.max = other.max
			pass
		return self
	def to_dict(self):
		return {
			"count":self.count,
			"sum":self.sum,
			"min":self.min,
			"max":self.max,
			"mean": self.sum / self.count,
			"unique_values": self.unique_values,
			}
	pass
class IntValueTypeTracker(NumericValueTypeTracker):
	pass
class FloatValueTypeTracker(NumericValueTypeTracker):
	pass
class LenthyValueTypeTracker(NumericValueTypeTracker):
	def track(self,value):
		return super().track(len(value))
	def to_dict(self):
		return {
			"count":self.count,
			"min_length":self.min,
			"max_length":self.max,
			"mean_length": self.sum / self.count,
			"unique_lengths": self.unique_values,
			}
	pass
class StrValueTypeTracker(NumericValueTypeTracker):
	def track(self,value):
		return super().track(value,len(value))
	def to_dict(self):
		return {
			"count":self.count,
			"length_sum":self.sum,
			"min_length":self.min,
			"max_length":self.max,
			"mean_length": self.sum / self.count,
			"unique_values": self.unique_values,
			}
	pass
class ListValueTypeTracker(LenthyValueTypeTracker):
	_name = "array"
	pass
class DictValueTypeTracker(LenthyValueTypeTracker):
	def track(self,value):
		super().track(value.keys())
		pass
	pass

class AttributeTracker:
	_value_type_map = {
		"list": ListValueTypeTracker,
		"bool": BoolValueTypeTracker,
		"dict": DictValueTypeTracker,
		"float": FloatValueTypeTracker,
		"int": IntValueTypeTracker,
		"NoneType": NoneValueTypeTracker,
		"str": StrValueTypeTracker,
		}
	def __init__(self,parent,name):
		self.parent = parent
		self.name = name
		self.count = 0
		self.value_trackers = {}
		pass

	def reset(self):
		self.parent = None
		self.count = 0
		self.value_trackers = {}
		pass

	def get_type_tracker(self,type_str):
		if type_str not in AttributeTracker._value_type_map:
			raise ValueError(f"Unknown type='{type_str}' for value='{value}'")
		if type_str not in self.value_trackers:
			self.value_trackers[type_str] = AttributeTracker._value_type_map[type_str](self,type_str)
			pass
		return self.value_trackers[type_str]

	def track(self,value):
		self.count += 1
		self.get_type_tracker(type(value).__name__).track(value)
		return self

	def merge(self,other):
		self.count += other.count
		for type_str,type_tracker in other.value_trackers.items():
			my_tracker = self.get_type_tracker(type_str)
			my_tracker.merge(type_tracker)
			pass
		pass

	def to_dict(self):
		value_trackers = {}
		for type_str,type_tracker in self.value_trackers.items():
			value_trackers[type_tracker.name] = type_tracker.to_dict()
			pass
		return {
			"count":self.count,
			"value_types":value_trackers,
			"prevalence": self.count / self.parent.object_count,
			}
	pass

class RedditBase(MyInfo):
	def __init__(self):
		super().__init__()
		self.object_count = 0
		self.files = []
		self.attribute_trackers = {}
		self.bytes_total = 0
		self.bytes_read = 0
		self.t0 = datetime.datetime.now()
		pass
	def get_attribute_tracker(self,attribute_name):
		if attribute_name not in self.attribute_trackers:
			self.attribute_trackers[attribute_name] = AttributeTracker(self,attribute_name)
			pass
		return self.attribute_trackers[attribute_name]

	def track_attrib(self,attribute_name,attribute_value):
		self.get_attribute_tracker(attribute_name).track(attribute_value)
		return self

	def to_dict(self):
		attrs = {}
		for attribute_name,attribute_tracker in self.attribute_trackers.items():
			attrs[attribute_name] = attribute_tracker.to_dict()
			pass
		my_dict = {
			"attributes":attrs,
			"object_count": self.object_count,
			"input_files": self.files,
			"seconds": ( datetime.datetime.now() - self.t0 ).total_seconds(),
			"bytes_total": self.bytes_total,
			"bytes_read": self.bytes_read,
			"t0": self.t0.strftime("%y-%m-%d %H:%M:%S"),
			}
		return my_dict

	def save_json_meta(self,last_update=False):
		print(f"Saving info to '{self.json_file}'")
		with open(self.json_file,'w') as fh:
			obj = self.to_dict()
			if last_update:
				obj["finished"] = True
				pass
			try:
				json.dump(obj,fh,indent=2,sort_keys=True)
			except:
				print(obj)
				raise
			pass
		return self

	pass

class RedditFile(RedditBase):
	"""Class to process one file in the format found on pushshift.io.

	It will create a JSON-based meta file which contains information on
	all of the types of attributes found for the particular type of file
	(tested on RC (Reddit Comments) and RS (Reddit Submission) files.

	When finised, it will pass the information to the parent so that an
	aggregate meta file for all files can be created.
	"""
	_ext2tool = {
		".bz2": ["bunzip2","-q","-c"],
		".zst": ["unzstd","-q","-c"],
		".gz": ["gunzip","-q","-c"],
		".xz": ["gunxz","-q","-c"],
		}
	def __init__(self,parent,filename):
		super().__init__()
		self.parent = parent
		self.args = parent.args
		self.filename = filename
		self.short_filename = os.path.basename(filename)
		self.files = [self.short_filename]
		self.full_filename = os.path.realpath(os.path.join(filename))
		self.base, self.extension = os.path.splitext(self.full_filename)
		print(f" - Will process '{self.full_filename}'.")
		self.file_size = os.path.getsize(self.full_filename)
		if self.extension in RedditFile._ext2tool:
			self.cmd = RedditFile._ext2tool[self.extension]
		else:
			self.cmd = None
			self.bytes_total = self.file_size
			pass
		self.ext = self.extension
		self.json_file = f"{self.base}-meta2.json"
		self.processed = False
		self.last_save_count = 0
		pass

	def process_line(self,line):
		"""Process one input line. The pushshift.io files all seem to

		have a psuedo-JSON format where each line is one object.

		This function processes one such input and adds its info to the
		meta object.

		"""
		self.bytes_read += len(line)
		self.object_count += 1
		self.parent.record_count += 1
		if 0 == self.object_count % self.args.min_record_check:
			self.update_progress()
			pass
		try:
			self.count_attribs(json.loads(line))
		except json.decoder.JSONDecodeError as e:
			print("ERROR on line %s" %(self.object_count))
			print("Line:\n%s" %(line))
			self.update_progress()
			raise e
		return self

	def process(self):
		""" Process the whole file, one line at a time."""
		self.t0 = datetime.datetime.now()
		# If we need to un-compress the file, do so using the appropriate
		# command and read in from STDIN.
		if self.cmd:
			return self.proc_from_cmd()
		# Otherwise, just read the file directly and process.
		with open(self.full_filename,'r') as fh:
			return self._proc_fh(fh)
		return self
	pass

	def proc_from_cmd(self):
		""" Process STDOUT from a command (e.g., uncompress) """
		# Don't process a file twice. Not sure why I thought I needed to check,
		# but much of this code is cut-and-paste from previous code used in
		# other research.
		if self.processed:
			return self
		# Prepare the command (append the file name to process
		self.cmd = self.data["subprocess"]
		self.cmd.append(self.data["file_name"])
		if args.verbosity > 0:
			print(f"Running: {self.cmd}")
			pass
		proc = subprocess.Popen(
			self.cmd,
			stdout=subprocess.PIPE,
			universal_newlines=True
			)
		# Now process like any other file handle.
		return self._proc_fh(proc.stdout)

	def _proc_fh(self,fh):
		""" Process the file once we have the file handle from which to read."""
		for line in fh:
			try:
				self.process_line(line)
			except json.decoder.JSONDecodeError as e:
				err = f"ERROR in JSON format, aborting load for '{self.filename}'."
				print(err)
				self.parent.errors(err)
				return self.save_progress(last_update=True)
			pass
		return self.save_progress(last_update=True)

	def count_attribs(self,obj):
		for key,value in obj.items():
			#size = len(str(value))
			self.track_attrib(key,value)
			#self.track_attrib(key,value,size)
			pass
		pass

	def update_progress(self):
		count = self.object_count
		args = self.args
		tn = datetime.datetime.now()
		secs = (tn - self.parent.tn).total_seconds()
		if secs < args.min_save_secs:
			return self
		if args.save_progress_each and count - self.last_save_count >= args.save_progress_each:
			self.last_save_count = count
			self.save_progress()
			pass
		self.parent.tn = tn
		num = self.pnum(count)
		if not self.cmd:
			rate = self.rate(self.bytes_read,total=self.file_size,append="bytes")
		else:
			rate = self.rate(count,total=args.num_records)
			pass
		print(f"{self.short_filename}: {self.psecs()} for {num} records. {rate}")
		self.parent.update_progress()
		return self


	def save_progress(self, last_update=False):
		if last_update:
			#self.parent.update_progress()
			self.save_json_meta(last_update=last_update)
			self.parent.merge_file_trackers(self)
		elif self.object_count - self.last_save_count >= self.args.save_progress_each:
			self.last_save_count = self.object_count
			pass
		return self


class RedditFileGroup(RedditBase):

	def __init__(self,args):
		super().__init__()
		self.args = args
		self.directory = args.directory
		self.json_file = args.outfile
		self.tn = datetime.datetime.now()
		self.file_count = 0
		self.total_size = 0
		self.record_count = 0
		self.file_objs = []
		self.errs = []
		self.jso_file = args.outfile
		for fname in self.args.files:
			if "*" in fname:
				for fname in glob.glob(fname):
					self.prep_file(fname)
					pass
				pass
			else:
				self.prep_file(fname)
				pass
			pass
		if self.directory is not None:
			self.prep_dir(self.directory)
			pass
		print(f"Will process a total of {self.file_count} files for {self.pbytes(self.total_size)}.")
		pass

	def errors(self,msg):
		self.errs.append(msg)
		return self

	def merge_file_trackers(self,psfmeta):
		self.files = []
		self.object_count = 0
		self.bytes_total = 0
		self.bytes_read = 0
		del(self.attribute_trackers)
		self.attribute_trackers = {}
		for file_tracker in self.file_objs:
			self.object_count += file_tracker.object_count
			self.bytes_total += file_tracker.bytes_total
			self.bytes_read += file_tracker.bytes_read
			self.files += file_tracker.files
			for attribute_name, tracker in file_tracker.attribute_trackers.items():
				self.get_attribute_tracker(attribute_name).merge(tracker)
				pass
			pass
		self.save_json_meta()
		return self

	def prep_file(self,filename):
		psfile = RedditFile(self,filename)
		self.file_objs.append(psfile)
		self.file_count += 1
		self.total_size += psfile.file_size
		pass

	def prep_dir(self,directory):
		if not os.path.exists(self.directory):
			print("ERROR: Could not find '%s'" %(self.directory))
			exit()
			pass
		print(f"Working on '{directory}' ...")
		for (dirpath,dirnames,filenames) in os.walk(directory):
			for filename in filenames:
				full_filename = os.path.realpath(os.path.join(dirpath,filename))
				self.prep_file(full_filename)
			pass
		pass

	def process(self):
		self.t0 = datetime.datetime.now()
		for ofile in self.file_objs:
			ofile.process()
			pass
		pass

	def update_progress(self):
		if len(self.file_objs) < 2:
			return self
		can_know_input_bytes = True
		self.bytes_read = 0
		self.object_count = 0
		for psfmeta in self.file_objs:
			self.bytes_read += psfmeta.bytes_read
			self.object_count += psfmeta.object_count
			if psfmeta.cmd:
				can_know_input_bytes = False
				pass
			pass
		args = self.args
		tn = datetime.datetime.now()
		secs = (tn - self.tn).total_seconds()
		num = self.pnum(self.object_count)
		if can_know_input_bytes:
			rate = self.rate(self.bytes_read,total=self.total_size,append="bytes")
		else:
			rate = self.rate(self.object_count,total=args.num_records)
			pass
		print(f"Overall: {self.psecs()} for {num} records. {rate}")
		return self


def main():
	parser = argparse.ArgumentParser(description='Build info about data from pushshift.io Reddit files.')
	parser.add_argument('files', nargs='+', help="Files to process.")
	parser.add_argument('--debug', '-d', default=False, action='store_true', dest='debug', help="Turn debugging on.")
	parser.add_argument('--dir', '-D', default=None, dest='directory', help="Process files in this directory.")
	parser.add_argument('--min-record-check', '-mr', default=100, dest='min_record_check', type=int,help="To speed things up, we only check if --min-save-secs after this many thousand records. Default: 5 (i.e., 5,000)")
	parser.add_argument('--min-save-secs', '-ms', default=60, dest='min_save_secs', type=int,help="The minimum number of secods which must pass before we save progress. Default: 10")
	parser.add_argument('--num-records', '-nr', default=None, dest='num_records', type=int,help="Number of records expected. Used to estimate time remaining.")
	parser.add_argument('--out', '-o', default=None, dest='outfile', help="Save aggregated JSON meta data to this file.")
	parser.add_argument('--quiet', '-q', default=0, dest='quiet', action='count',help="Decrease verbosity. Can have multiple.")
	parser.add_argument('--save-progress-each', '-sp', default=10, dest='save_progress_each', type=int,help="Number of --min-record-check for printing what we have so far with attribute information.")
	parser.add_argument('--verbose', '-v', default=0, dest='verbosity', action='count',help="Increase verbosity. Can have multiple.")

	args = parser.parse_args()
	args.verbosity = 1 + args.verbosity - args.quiet
	args.min_record_check = args.min_record_check * 1000
	if args.save_progress_each is not None:
		args.save_progress_each = args.save_progress_each * args.min_record_check
		pass
	if args.verbosity > 0:
		print(f"Settings:")
		print(f" - {me.pnum(args.min_record_check)} lines before checking for save or print.")
		print(f" - {me.psecs(args.min_save_secs)} minimum between progress output.")
		print(f" - Save/print after {me.pnum(args.save_progress_each)} records if {me.psecs(args.min_save_secs)} have passed.")
		pass
	args.timestr = time.strftime("%Y%m%d-%H%M%S")
	pinfo = RedditFileGroup(args)
	if len(pinfo.files) > 1:
		if args.outfile is None:
			print("ERROR: --out required if more than one file.")
			exit(1)
			pass
		pass
	pinfo.process()
	pass

if __name__ == "__main__":
	main()
	sys.exit(0)
	pass
